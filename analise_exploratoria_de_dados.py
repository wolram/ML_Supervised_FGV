# -*- coding: utf-8 -*-
"""Analise_Exploratoria_de_Dados

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TVJT8ZHzyOVQsLEdQTk_602gIty_poCf

*Professora Ana Gularte*  
Disciplina: **Técnicas Supervisionadas de Machine Learning**

---

# Objetivos da Aula

- Discutir a importância da EDA como etapa fundamental antes da modelagem preditiva.  
- Importar e compreender bibliotecas essenciais para análise de dados em Python.  
- Explorar a estrutura e as características do dataset **Automobile (UCI)**.  
- Caracterizar variáveis segundo seus tipos (qualitativas nominais/ordinais, quantitativas discretas/contínuas) e escalas de medida.  
- Gerar e interpretar estatísticas descritivas (média, mediana, desvio padrão, etc.).  
- Avaliar medidas de forma da distribuição: obliquidade (*skewness*) e curtose.  
- Aplicar testes de normalidade (ex.: Shapiro–Wilk) em variáveis contínuas.  
- Analisar variáveis categóricas (contagem, frequência percentual, comparação por grupos).  
- Visualizar distribuições de variáveis por meio de gráficos (histogramas, boxplots, gráficos de pizza, KDE plots, FacetGrid, etc.).  
- Identificar e interpretar outliers utilizando boxplots e limites IQR.  
- Explorar relações entre variáveis por meio de covariância, correlação e scatter plots.  
- Construir visualizações gráficas avançadas (pairplots, heatmaps, etc.) para apoiar insights.  
- Aplicar e interpretar Análise de Componentes Principais (PCA) para redução de dimensionalidade.  
- Avaliar a variância explicada pelos componentes principais e interpretar os *loadings*.  
- Discutir potenciais problemas de multicolinearidade e seus impactos na análise.  
- Gerar relatórios automáticos de Análise Exploratória de Dados com ferramentas como **ydata-profiling** e **Sweetviz**.  
- Comparar vantagens e limitações entre abordagens de EDA manual e automatizada.

# Análise Exploratória de Dados (EDA - *Exploratory Data Analysis*)

Muitas literaturas e cursos de aprendizado de máquina costumam começar direto pelos **modelos, técnicas e algoritmos**.  
Porém, existe um passo essencial que antecede qualquer aplicação de machine learning: **entender os dados**.  

Sem essa análise prévia, corremos o risco de trabalhar com informações incompletas, distorcidas ou de baixa qualidade.  
A **Análise Exploratória de Dados (EDA)** é justamente o momento em que buscamos:  
- Compreender a base de dados.  
- Identificar padrões.  
- Analisar distribuições.  
- Verificar relações entre variáveis.  

Essa etapa é crucial porque **a qualidade do modelo depende diretamente da qualidade dos dados**.  
Se os dados não são consistentes, dificilmente teremos resultados confiáveis, independentemente da sofisticação da técnica utilizada.  

No mundo corporativo, esse aprendizado é ainda mais importante.  
Muitas empresas investem em sistemas de aprendizado de máquina, mas veem seus modelos falharem em produção.  
Isso acontece porque etapas fundamentais de exploração e diagnóstico dos dados foram negligenciadas.  
Pequenos problemas, que poderiam ser facilmente identificados no início, acabam comprometendo todo o projeto.  

**Grande lição:** antes de pensar em algoritmos, precisamos **explorar, entender e preparar os dados**.  
É nessa etapa que garantimos a base sólida para análises e modelos que realmente tragam valor e funcionem na prática.

## 1. Importar bibliotecas necessárias

Vamos importar as principais bibliotecas para fazer análise exploratória e manipulação de dados em Python: `pandas`, `numpy`, `matplotlib,pyplot`, `seaborn`, `ydata-profiling`, entre outras abaixo listadas.
"""

# Ignora avisos desnecessários durante a execução
import os, warnings
os.environ["PYTHONWARNINGS"] = "ignore::DeprecationWarning"
warnings.filterwarnings("ignore", category=DeprecationWarning)

# ==============================================
# Instalar bibliotecas necessárias (se precisar)
# ==============================================

!pip install -q seaborn scipy scikit-learn ydata-profiling sweetviz

# ============================
# Importar as bibliotecas
# ============================

# Manipulação e análise de dados (criação de DataFrames, leitura de CSV, etc.)
import pandas as pd

# Manipulação de vetores, matrizes e operações numéricas de alto desempenho
import numpy as np

# Teste estatístico de normalidade (Shapiro-Wilk)
from scipy.stats import shapiro

# Estatística descritiva, distribuições de probabilidade e testes de hipótese
import scipy.stats as stats

# Visualização básica de gráficos
import matplotlib.pyplot as plt

# Visualização estatística mais estilizada (baseada em matplotlib)
import seaborn as sns

# Geração automática de relatórios de Análise Exploratória de Dados (EDA)
from ydata_profiling import ProfileReport

# Path simplifica o trabalho com caminhos de arquivos e pastas
from pathlib import Path

# Tratamento de valores ausentes (NaN), substituir dados faltantes por média, mediana, moda ou um valor constante
# Preparar a base antes de aplicar algoritmos de machine learning ou análises estatísticas
from sklearn.impute import SimpleImputer

# Pré-processamento de dados: padronização (média = 0, desvio padrão = 1)
from sklearn.preprocessing import StandardScaler

# Importa a classe responsável por realizar a **Análise de Componentes Principais (PCA)**.
from sklearn.decomposition import PCA

# Define o estilo padrão de gráficos no seaborn
sns.set()

# Depois de executar, você vai autorizar o acesso e o Drive ficará acessível no caminho
from google.colab import files
from google.colab import drive
drive.mount('/content/drive')

"""## 2. Base de dados

Como mencionado na aula 1 em `Alguns repositórios de dados`, trabalharemos com o dataset [Automobile](https://archive.ics.uci.edu/ml/datasets/Automobile) da UCI.  Esta base contém características de diversos automóveis e o risco (em termos de seguro) de cada um deles.  Consideraremos que as bases estão armazenadas em um arquivo `CSV`.
"""

# ============================
# Carregar dados
# ============================

# Selecionar caminho para a pasta no drive
caminho_pasta = "/content/drive/MyDrive/AULA_FGV/Técnicas_Supervisionadas_de_Machine_Learning_FGV"

# Ler o arquivo `imports85.csv` em um DataFrame do pandas
data = pd.read_csv(f"{caminho_pasta}/imports85.csv")

# Visualizar a dimensão do DataFrame e as primeiras linhas
print(data.shape)
data.head()

"""Quais são os atributos (*features*)?"""

X = data.drop(columns=['symboling', 'normalizedlosses'])
print(X.head())
print(X.shape)

"""Qual é o atributo alvo?"""

y = data.symboling
print(y)

"""## 3. Caracterização de dados

### Tipos de dados (dtypes no Pandas)

Nesta etapa, observamos como o **pandas** reconhece os dados do DataFrame por meio dos *dtypes* (tipos de dados).  
- **object**: normalmente usado para representar variáveis categóricas (texto).  
- **int64**: números inteiros, em geral associados a contagens ou variáveis discretas.  
- **float64**: números decimais, usados para variáveis contínuas.  

Essa visão é importante para garantir que cada coluna esteja corretamente tipada antes de avançarmos para análises estatísticas mais detalhadas, onde classificaremos os atributos como qualitativos (nominais, ordinais) ou quantitativos (discretos, contínuos).
"""

X.dtypes

"""No `pandas`, por padrão, valores:

- qualitativos (nominais e ordinais) são armazenados como `object`;
- quantitativos discretos, como `int64`;
- quantitativos contínuos, como `float64`.

Informações, como:

- unidades associadas (quantitativos),
- ordem (qualitativos ordinais),

devem ser tratadas pelo conhecimento de domínio.

### Tipos de atributos

### Qualitativos nominais

*Será?* O que o especialista diria?
"""

print('make', pd.unique(X.make))

print('fueltype', pd.unique(X.fueltype))

print('aspiration', pd.unique(X.aspiration))

print('bodystyle', pd.unique(X.bodystyle))

print('drivewheels', pd.unique(X.drivewheels))

print('enginelocation', pd.unique(X.enginelocation))

print('enginetype', pd.unique(X.enginetype))

print('fuelsystem', pd.unique(X.fuelsystem))

"""### Qualitativos ordinais"""

print('numofdoors', pd.unique(X.numofdoors))

print('numofcylinders', pd.unique(X.numofcylinders))

"""### Quantitativos discretos

Podemos converter `numofcylinders` e `numofdoors`.

### Quantitativos contínuos
"""

print('wheelbase', X.wheelbase.min(), X.wheelbase.max())

print('length', X.length.min(), X.length.max())

print('width', X.width.min(), X.width.max())

print('height', X.height.min(), X.height.max())

print('curbweight', X.curbweight.min(), X.curbweight.max())

print('enginesize', X.enginesize.min(), X.enginesize.max())

print('bore', X.bore.min(), X.bore.max())

print('stroke', X.stroke.min(), X.stroke.max())

print('compressionratio', X.compressionratio.min(), X.compressionratio.max())

print('horsepower', X.horsepower.min(), X.horsepower.max())

print('peakrpm', X.peakrpm.min(), X.peakrpm.max())

print('citympg', X.citympg.min(), X.citympg.max())

print('highwaympg', X.highwaympg.min(), X.highwaympg.max())

print('price', X.price.min(), X.price.max())

"""### Escala

Precisaríamos de um especialista para decidir se são intervalares ou racionais.

Qual seria sua opinião?

## 4. Exploração de dados

### Estatística Descritiva  

- **Definição:** conjunto de técnicas que resumem e descrevem, de forma quantitativa e visual, as principais características de um conjunto de dados.  
- Permite identificar padrões, tendências, anomalias e direcionar etapas posteriores da análise.  

**Principais dimensões avaliadas:**  
- **Frequência:** contagem e proporção de categorias ou ocorrências (ex.: % de clientes em cada região).  
- **Tendência central:** medidas como **média, mediana e moda**, que sintetizam valores típicos.  
- **Dispersão:** grau de variação dos dados em torno da média, medido por **variância, desvio-padrão, amplitude, IQR**.  
- **Forma da distribuição:** assimetria (skewness) e curtose, que indicam se os dados são enviesados ou têm caudas pesadas.  
- **Valores atípicos:** pontos que se afastam do padrão geral e podem indicar erros ou insights relevantes.  

**Importância na prática:**  
- Serve como **primeiro diagnóstico** sobre a qualidade e estrutura dos dados.  
- Fundamenta a escolha de técnicas de **pré-processamento** (tratamento de outliers, normalização, imputação de faltantes).  
- Guia a aplicação de **modelos estatísticos e de aprendizado de máquina**, ao revelar suposições que precisam ser atendidas.

### Análise Univariada

### Frequencia

- **Definição:** proporção de vezes que um atributo assume um dado valor.  
- **Aplicação:** utilizada em valores numéricos e simbólicos.  

**Exemplo – variável `fueltype`:**  
- **gas:** 90,2%  
- **diesel:** 9,8%
"""

print(X.fueltype.value_counts())

"""Fazer para os outros atributos...

### Localidade ou Tendência Central

- **Definição:** medidas que definem pontos de referência nos dados, representando um valor “típico” que resume o conjunto.  

**Valores numéricos:**  
- Média: medida de tendência central mais usada. Calculada como a soma de todos os valores dividida pelo número de observações.
- Mediana: valor que ocupa a posição central quando os dados estão ordenados.
  - A média é bastante sensível a um outlier único, enquanto a mediana é robusta a valores extremos, desde que eles não constituam a maioria dos dados.
- Percentil: pontos que dividem os dados em partes iguais (ex.: o percentil 90 indica o valor abaixo do qual estão 90% das observações).
- Etc.  

**Valores simbólicos:**  
- Moda: valor que ocorre com maior frequência.
"""

print(X.price.mean())

print(X.price.median())

print(X.price.quantile(q = 0.75))

print(X.make.mode())

"""### Boxplots

- Também chamados de **diagramas de Box e Whisker**.  
- Forma gráfica de visualizar **quartis**.  
- Utilizam quartis e valores máximo e mínimo.  

**Elementos principais:**  
- **Q1:** Primeiro quartil.  
- **Q2:** Mediana (segundo quartil).  
- **Q3:** Terceiro quartil.  
- **Extremos:** valores mínimo e máximo (dentro do limite aceitável).  

---

### Boxplot Modificado

- O limite superior/inferior vai até o maior/menor valor apenas se esse valor não for muito distante do 3º/1º quartil.  
- O critério utilizado é **1,5 × IQR (intervalo interquartílico)**, onde:  

$$
IQR = Q3 - Q1
$$  

- Valores acima ou abaixo desse limite são considerados **outliers**.

"""

# Criar dados simulados
np.random.seed(42)
dados = np.random.normal(loc=50, scale=10, size=200)

# Calcular estatísticas
q1 = np.percentile(dados, 25)
q2 = np.median(dados)
q3 = np.percentile(dados, 75)
minimo = np.min(dados)
maximo = np.max(dados)

# Criar boxplot
plt.boxplot(dados, vert=False, patch_artist=True, boxprops=dict(facecolor="yellow"))
plt.title("Exemplo de Boxplot")
plt.xlabel("Valores")

# Adicionar rótulos
plt.text(minimo, 1.1, "mínimo", ha="center")
plt.text(q1, 1.1, "Q1", ha="center")
plt.text(q2, 1.1, "Q2", ha="center")
plt.text(q3, 1.1, "Q3", ha="center")
plt.text(maximo, 1.1, "máximo", ha="center")

plt.show()

"""### Quartis e Percentis

- **Mediana:** divide os dados ordenados ao meio.  
- **Quartis e percentis:** utilizam pontos de divisão diferentes.  

**Quartis:**  
- **1º quartil (Q1):** valor que tem 25% dos demais valores abaixo dele.  
- **2º quartil (Q2):** mediana (50% dos valores abaixo).  
- **3º quartil (Q3):** valor que tem 75% dos demais valores abaixo.  

**Percentis:**  
- Definidos para *p* entre 0 e 100.  
- O *p*-ésimo percentil (Pp) é o valor `xi` tal que *p%* dos valores observados são menores que `xi`.  

**Relações importantes:**  
- P25 = Q1  
- P50 = Q2 = mediana  
- P75 = Q3
"""

plt.boxplot(X.price[~np.isnan(X.price)]) # Removemos os atributos ausentes
plt.show()

"""### Medidas de Espalhamento  

- **Definição:** indicam o grau de **dispersão** ou **variabilidade** de um conjunto de valores.  
- **Utilidade:** ajudam a entender se os dados estão:  
  - Muito **espalhados** (alta variabilidade).  
  - Mais **concentrados** em torno de um valor central (ex.: a média).  

**Medidas mais comuns:**  
- **Intervalo (ou amplitude):** diferença entre o maior e o menor valor.  
- **Variância:** média dos desvios quadráticos em relação à média.  
- **Desvio padrão:** raiz quadrada da variância; expressa a dispersão na mesma unidade dos dados.
"""

print(X.price.max() - X.price.min())

print(X.price.var())

print(X.price.std())

"""### Medidas de Distribuição

**Definição:** Obliquidade (Skewness) e Curtose (Kurtosis) descrevem a forma da distribuição dos dados em torno da média.

---

### Obliquidade (Skewness)
- Mede a **simetria da distribuição** em torno da média.  
- Distribuições podem ser:
  - **Simétrica** → skew ≈ 0 (ex.: distribuição normal)  
  - **Assimétrica à esquerda (negativa)** → cauda mais longa à esquerda  
  - **Assimétrica à direita (positiva)** → cauda mais longa à direita  
- Valores de referência:
  - `|skew| < 0.5` → aproximadamente simétrica  
  - `0.5 ≤ |skew| < 1` → moderadamente assimétrica  
  - `|skew| ≥ 1` → fortemente assimétrica  

---

### Curtose (Kurtosis)
- Mede o **grau de concentração nas caudas** em relação à distribuição normal.  
- Tipos comuns:
  - **Mesocúrtica** → curtose ≈ 0 (ex.: normal)  
  - **Leptocúrtica** → curtose > 0 (caudas pesadas, mais outliers)  
  - **Platicúrtica** → curtose < 0 (caudas leves, menos outliers)
"""

print(X.price.skew())

print(X.price.kurtosis())

"""### Histograma

- **Definição:** forma gráfica para visualizar a distribuição de dados.  
- **Funcionamento:** divide os valores em *cestas* (bins).  

**Regras de formação das cestas:**  
- **Valores categóricos:** cada valor único corresponde a uma cesta.  
- **Valores numéricos:** divisão em intervalos contíguos de mesmo tamanho, cada intervalo é uma cesta.  

**Representação:**  
- Para cada cesta, desenha-se uma barra cuja altura é proporcional ao número de elementos naquela faixa.
"""

# Distribuição de uma variável quantitativa contínua
plt.hist(X.price[~np.isnan(X.price)], 50)
plt.show()

# tira valores NaN
dados = X.price[~np.isnan(X.price)]

plt.figure(figsize=(8,5))
# histograma com densidade
sns.histplot(dados, bins=50, kde=True, stat="density", color="skyblue", edgecolor="black")

# curva normal ajustada
mu, sigma = dados.mean(), dados.std()
x = np.linspace(dados.min(), dados.max(), 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma), 'r', lw=2, label="Normal teórica")

plt.title(f"Distribuição de Preço\nMédia={mu:.2f}, Desvio={sigma:.2f}, Curtose={dados.kurtosis():.2f}")
plt.legend()
plt.show()

"""### Interpretação da Distribuição de Preço

1. Comparação: dados reais × normal teórica
- **Barras azuis** → histograma dos preços  
- **Linha azul clara** → curva de densidade (KDE) com base nos dados  
- **Linha vermelha** → curva normal teórica (mesma média e desvio-padrão)  

Os dados **não seguem bem a normal**:
- Maior concentração entre **7k–15k**  
- **Cauda longa à direita** (valores altos mais frequentes do que na normal)

---

2. Estatísticas principais
- **Média** ≈ 13.207  
- **Desvio-padrão** ≈ 7.947  
- **Curtose (excesso)** = 3.23 → indica **caudas pesadas** e presença de outliers

---

3. Implicações práticas
- Distribuição **assimétrica à direita**  
- **Não é normal**: concentração em valores médios-baixos + muitos extremos  
- Para análises/modelos que assumem normalidade:  
  - Testar transformações (log, Box-Cox, Yeo-Johnson)  
  - Usar métodos robustos a outliers  

---

> **Conclusão**: `price` apresenta distribuição **não-normal**, com forte cauda à direita e curtose elevada.
"""

import numpy as np
from scipy.stats import shapiro

# tira valores NaN
dados = X.price[~np.isnan(X.price)]

# aplica o teste Shapiro-Wilk
stat, p = shapiro(dados)

print(f"Stat={stat:.4f}, p={p:.4f}")
if p > 0.05:
    print("Não rejeita normalidade")
else:
    print("Rejeita normalidade")

"""### Teste de Normalidade (Shapiro-Wilk)

**Definição:**  
O teste **Shapiro-Wilk** avalia a hipótese nula de que os dados vêm de uma **distribuição normal**.  

- **Estatística (W):** varia entre 0 e 1 → quanto mais próximo de 1, mais os dados se parecem com a normal.  
- **p-valor:** indica se rejeitamos ou não a hipótese de normalidade.  

**Critério de decisão:**  
- p > 0.05 → **não rejeita normalidade** (dados compatíveis com normal)  
- p ≤ 0.05 → **rejeita normalidade** (dados não seguem normalidade)  

---

**Resultado obtido**
- **Stat = 0.7985**  
- **p = 0.0000**  
- Decisão: **Rejeita normalidade**


> Isso confirma que a variável `price` **não segue distribuição normal**, o que é coerente com a assimetria positiva (skewness = 1.81) e a curtose alta (3.23) observadas anteriormente.

### Distribuição da Variável Categórica (fueltype)

Além de analisar a variável numérica `price`, também é útil visualizar a distribuição de variáveis **categóricas**.  
O histograma (ou countplot) mostra a frequência de cada categoria presente na variável `fueltype`.
"""

plt.hist(X.fueltype)
plt.show()

"""### Gráfico de Pizza

- **Definição:** outra forma gráfica de visualizar a distribuição de um conjunto de valores.  
- **Indicação:** mais adequado para **valores qualitativos**.  
- **Para valores quantitativos:** é necessário agrupar em cestas (intervalos).  
- **Representação:** cada valor ocupa uma fatia cuja área é proporcional ao número de vezes que aparece no conjunto de dados.  

**Exemplo:**  
Um bom uso seria a variável **`fueltype`**, que possui duas categorias:  
- **gas:** 90,2%  
- **diesel:** 9,8%

Nesse caso, o gráfico de pizza mostra de forma clara a proporção de carros a gasolina versus a diesel.
"""

# Contagem dos tipos de combustível
fuel_counts = X["fueltype"].value_counts()

# Criar gráfico de pizza
plt.figure(figsize=(6,6))
plt.pie(fuel_counts, labels=fuel_counts.index, autopct='%1.1f%%',
        startangle=90, colors=["gold", "lightblue"])
plt.title("Distribuição de tipo de combustível")
plt.show()

"""### `Pandas.DataFrame.describe()`

O método `describe()` do **pandas** gera um resumo estatístico automático das colunas numéricas (e, opcionalmente, categóricas) de um DataFrame.  

Esse resumo é útil na **Análise Exploratória de Dados (EDA)** porque permite ter uma visão rápida das principais estatísticas descritivas, como:  

- **count** → número de valores não nulos  
- **mean** → média  
- **std** → desvio padrão  
- **min, max** → valores mínimo e máximo  
- **25%, 50%, 75%** → quartis (medidas de posição que ajudam a entender a dispersão dos dados)  

Para variáveis **categóricas**, podemos usar:  

- `df.describe(include=["object"])` → gera estatísticas apenas para variáveis qualitativas.  
- `df.describe(include="all")` → gera estatísticas para todos os tipos de variáveis.  

Assim, é possível ter uma visão geral tanto de variáveis numéricas quanto de variáveis qualitativas de forma rápida e prática.

"""

# Resumo estatístico numérico
X.describe()

# Incluir colunas categóricas também
X.describe(include="all")

# Resumo estatístico + skewness + kurtosis
stats = X["price"].agg(["mean", "median", "std", "min", "max"])
stats["skew"] = X["price"].skew()
stats["kurtosis"] = X["price"].kurtosis()  # excesso de curtose (normal = 0)

# Exibir em formato tabular
stats = stats.to_frame(name="price")
stats

# Variável qualitativa
stats_cat = X["fueltype"].describe()[["count", "unique", "top", "freq"]]

# Exibir em formato tabular
stats_cat = stats_cat.to_frame(name="fueltype")
stats_cat

# Distribuição de frequências relativas (%)
freq_table = X["fueltype"].value_counts(normalize=True).to_frame(name="proporção")
freq_table

"""### Dados Multivariados

- **Definição:** possuem mais de um atributo de entrada.  


**Medidas estatísticas:**  
- Medidas de localidade e espalhamento podem ser calculadas separadamente para cada atributo.  
  - Exemplo: média  
  - Média em dados multivariados:  
    $$
    \bar{x} = (\bar{x}_1, \ldots, \bar{x}_d)
    $$  

**Análises possíveis:**  
- Permitem estudar a relação entre dois ou mais atributos.  
- Para variáveis contínuas, o espalhamento é melhor capturado por uma **matriz de covariância**.  
- Cada elemento da matriz representa a covariância entre dois atributos.  

**Covariância:**  
$$
\text{cov}(x_i, x_j) = \frac{1}{n - 1} \sum_{k=1}^{n} (x_{ik} - \bar{x}_i)(x_{jk} - \bar{x}_j)
$$  

**Observação:**  
$$
\text{cov}(x_i, x_i) = \text{var}(x_i)
$$

### Covariância

- **Definição:** a covariância entre dois atributos mede o grau com que eles variam juntos.  
- **Interpretação dos valores entre dois vetores** $x_i$ e $x_j$:  
  - **Próximo de 0:** atributos não têm um relacionamento linear.  
  - **> 0 (positiva):** atributos são diretamente relacionados.  
  - **< 0 (negativa):** atributos são inversamente relacionados.  

**Atenção:**  
- O valor depende da **magnitude dos atributos**.  
- Não é possível avaliar o relacionamento entre atributos apenas pela covariância.
"""

Xnumeric = X.select_dtypes(include=['int64', 'float64'])

np.cov(Xnumeric)

"""### Correlação

A **correlação** indica a força da relação linear entre dois atributos.

- **Matriz de correlação**: mostra a correlação entre todos os pares de atributos.

### Fórmula da Correlação de Pearson

$$
\rho(x_i, x_j) = \frac{\mathrm{cov}(x_i, x_j)}{\sigma_{x_i}\,\sigma_{x_j}}
$$

onde:  
- $\mathrm{cov}(x_i, x_j)$ = covariância entre $x_i$ e $x_j$  
- $\sigma_x$ = desvio-padrão da variável $x$  

---

**Observações:**  
- Valores em $[-1,\,1]$:  
  - $-1$ → correlação linear negativa máxima  
  - $+1$ → correlação linear positiva máxima  
  - $0$ → ausência de correlação linear  
- Autocorrelação: $\rho(x_i, x_i) = 1$  
- Valores próximos de $-1$ ou $+1$ indicam **forte correlação linear**.  
- Importante: a correlação de Pearson mede apenas **relações lineares**.  
  Uma correlação próxima de 0 não implica ausência total de relação — pode haver relação **não linear**.

---

> **Atenção**  
É tentador supor que uma alta correlação implique algum tipo de conexão causal, mas isso está errado.  
Correlação indica apenas **associação linear** entre variáveis, não necessariamente uma relação de causa e efeito.

---

### Exemplo de Correlação Espúria

Um exemplo clássico de correlação espúria é a relação entre:

- **Consumo de sorvete**   
- **Número de afogamentos**

Sorvete e afogamentos → correlação positiva.

Durante o verão, ambos aumentam, mas **não porque um causa o outro**.  
Na verdade, a variável escondida (**temperatura**) influencia os dois fenômenos.

---

> Essa situação ilustra que **correlação não significa causalidade**.  
É sempre necessário investigar fatores externos, considerar o contexto e verificar a influência de outras variáveis no problema.

"""

np.corrcoef(Xnumeric)

"""### Relação entre preço e potência

Um bom método padrão para descrever a relação entre duas variáveis é:

1. **Visualização** com diagrama de dispersão bivariado (scatter plot)
2. Medida de associação (coeficiente de correlação)
3. Estatísticas de resumo (média, SD, n, etc.)

"""

plt.scatter(X.price, X.horsepower)
plt.xlabel("Preço")
plt.ylabel("Potência (horsepower)")
plt.title("Relação entre Preço e Potência")
plt.show()

"""### Interpretação do Scatter: Preço × Potência

- **Tendência positiva**  
  - Carros com maior **potência (horsepower)** tendem a ter maior **preço**.  
  - Isso sugere uma **correlação positiva** entre as variáveis.  

- **Dispersão**  
  - Há carros com **preços semelhantes**, mas potências diferentes, e vice-versa.  

- **Possíveis outliers**  
  - Alguns modelos muito caros e potentes destoam do padrão, possivelmente carros esportivos ou de luxo.  

- **Conclusão**  
  - O scatter ajuda a **visualizar a relação geral**, mas não explica o preço sozinho:  
    outros fatores também influenciam o valor de um automóvel.
"""

X[["price", "horsepower"]].corr()

""" O coeficiente de correlação **0.81** indica uma **forte correlação positiva**:  
carros mais potentes tendem a ser mais caros.  

> No entanto, correlação não significa causalidade:  
o preço de um carro é determinado por múltiplos fatores além da potência (marca, design, tecnologia, etc.).
"""

X[["price", "horsepower"]].describe()

"""### Estatísticas de resumo: Preço × Potência

|       | price       | horsepower |
|-------|-------------|------------|
| count | 201         | 203        |
| mean  | 13.207      | 104.26     |
| std   | 7.947       | 39.71      |
| min   | 5.118       | 48         |
| 25%   | 7.775       | 70         |
| 50%   | 10.295      | 95         |
| 75%   | 16.500      | 116        |
| max   | 45.400      | 288        |

- O preço médio dos carros é de **≈13 mil dólares**, variando entre **5 mil** e **45 mil**.  
- A potência média é de **≈104 HP**, indo de **48 HP** a **288 HP**.  
- Os quartis mostram que a maior parte dos carros tem **potência abaixo de 120 HP** e **preço abaixo de 16.500**.  

> Esses valores ajudam a contextualizar o gráfico de dispersão e a correlação:  
carros mais potentes tendem a estar na faixa de preço mais alta.

### Distribuição de Preços por Tipo de Combustível KDE plot (Kernel Density Estimation)

- O gráfico mostra a **densidade de probabilidade** dos preços.  
- A **área sob cada curva é 1 (100%)**, permitindo comparar formatos mesmo com tamanhos de amostras diferentes.  
- **Gasolina (azul):** distribuição concentrada entre 5.000 e 20.000, com alguns picos em faixas mais altas (~30k–40k).  
- **Diesel (laranja):** preços mais concentrados em torno de 10.000–15.000, sem grande dispersão.  

> Esse tipo de gráfico é ideal para comparar **a forma da distribuição** entre grupos diferentes.
"""

sns.kdeplot(data=X, x="price", hue="fueltype", fill=True, alpha=0.3)

"""### Distribuição de Preços por Tipo de Combustível (Histograma)

- O gráfico mostra a **contagem absoluta** de carros em cada faixa de preço.  
- **Gasolina (azul):** muito mais observações que diesel, concentradas entre 5.000 e 20.000.  
- **Diesel (laranja):** menor quantidade de carros, também concentrados na faixa de 10.000–15.000.  
- A sobreposição das barras permite visualizar as diferenças de volume entre os grupos.  

> Esse tipo de gráfico é ideal para analisar **quantidades reais** de observações em cada categoria.

"""

plt.figure(figsize=(10,6))
sns.histplot(data=data, x="price", hue="fueltype", bins=50, alpha=0.5)
plt.title("Histograma do preço por tipo de combustível")
plt.xlabel("Preço")
plt.ylabel("Contagem")
plt.show()

"""### Histogramas de Preço por Tipo de Combustível (FacetGrid)

- O gráfico mostra **um histograma separado para cada categoria de combustível**.  
- **Gasolina (acima):** grande volume de observações, concentrado entre 5.000 e 20.000, com longa cauda à direita.  
- **Diesel (abaixo):** bem menos observações, concentradas em torno de 10.000–20.000, sem grande dispersão.  
- A separação dos gráficos facilita a leitura, evitando sobreposição de cores.  

> Esse tipo de visualização é útil para analisar **cada grupo individualmente**, sem perder a escala comparável do eixo X (preço).
"""

g = sns.FacetGrid(data, row="fueltype", height=4, aspect=2, sharey=False)
g.map(sns.histplot, "price", bins=50, color="steelblue")
g.set_axis_labels("Preço", "Contagem")
g.fig.subplots_adjust(top=0.9)
g.fig.suptitle("Histogramas do preço por tipo de combustível")
plt.show()

"""### Boxplot do Preço por Tipo de Combustível

- O boxplot permite comparar a **distribuição dos preços** entre gasolina e diesel.  
- **Gasolina (azul):**  
  - Mediana em torno de 10.000.  
  - Maior concentração de preços entre 7.500 e 15.000.  
  - Muitos outliers acima de 25.000, chegando até 45.000.  
- **Diesel (laranja):**  
  - Mediana mais alta (~14.000).  
  - Distribuição mais ampla, com preços entre 7.000 e 32.000.  
  - Poucos outliers.  

> Esse gráfico é ideal para **comparar medianas, dispersão e outliers** entre categorias.
"""

plt.figure(figsize=(8,6))
sns.boxplot(data=data, x="fueltype", y="price", hue="fueltype", palette="pastel", legend=False)
plt.title("Boxplot do preço por tipo de combustível")
plt.xlabel("Tipo de combustível")
plt.ylabel("Preço")
plt.show()

"""### Relatórios exploratórios automáticos

Vamos gerar relatórios de análise exploratória de dados utilizando `ydata-profiling` e `sweetviz`, desconsiderando a coluna `normalizedlosses`.
"""

# ============================================================
# Gerar e salvar Relatório Automático de EDA (ydata_profiling)
# e baixar para o computador
# ===========================================================

# Gerar o relatório
profile = ProfileReport(
    data.drop(columns=["normalizedlosses"], errors="ignore"),
    title="Relatório de Análise Exploratória",
    explorative=True
)

# Salvar o HTML em diretório definido
output_file = "/content/drive/MyDrive/AULA_FGV/Técnicas_Supervisionadas_de_Machine_Learning_FGV/Relatorio_Automatico_EDA.html"
profile.to_file(output_file)

# Fazer o download para o computador
files.download(output_file)

# =======================================================================
# Gerar e salvar Relatório Automático de EDA (ydata_profiling e sweetviz)
# e baixar para o computador
# =======================================================================

# --- Compatibilidade Sweetviz x NumPy>=2 (monkey-patch) ---
# Executar este bloco ANTES de "import sweetviz"
if not hasattr(np, "VisibleDeprecationWarning"):
    np.VisibleDeprecationWarning = DeprecationWarning

# ============================================
# Configurações básicas
# ============================================
# Caminho do CSV
# -> Atribua o caminho local/Drive.
DATA_PATH = "/content/drive/MyDrive/AULA_FGV/Técnicas_Supervisionadas_de_Machine_Learning_FGV/imports85.csv"   # <-- ajuste aqui se precisar

# Pasta de saída
OUT_DIR = Path("./eda_output")
OUT_DIR.mkdir(exist_ok=True)

# (Opcional) coluna alvo para análises supervisionadas
# Exemplo: TARGET_COL = "symboling"
TARGET_COL = None

# ============================================
# 1) Carregar dados e salvar "com índices"
# ============================================
df = pd.read_csv(DATA_PATH)

# Cria uma coluna explícita de índice (row_id = 1..N), preservando a ordem original
df_i = df.reset_index().rename(columns={"index": "row_id"})
df_i["row_id"] = df_i["row_id"] + 1

# Salva o dataset com a coluna de índice (sem adicionar índice do pandas no arquivo)
csv_com_indices = OUT_DIR / "dataset_com_indices.csv"
df_i.to_csv(csv_com_indices, index=False)

print(f"✅ Dataset salvo COM índices (coluna 'row_id'): {csv_com_indices.resolve()}")

# ============================================
# 2) Resumos descritivos (salvos COM índice)
# ============================================
# Estatísticas descritivas por coluna (sem datetime_is_numeric)
desc = df.describe(include="all").transpose()
desc_csv = OUT_DIR / "resumo_descritivo.csv"
desc.to_csv(desc_csv)  # mantém o índice (nomes das colunas) no arquivo
print(f"✅ Resumo descritivo salvo: {desc_csv.resolve()}")

# Valores ausentes por coluna
missing = df.isna().sum().to_frame("missing_count")
missing["missing_pct"] = (missing["missing_count"] / len(df)).round(4)
missing_csv = OUT_DIR / "valores_ausentes.csv"
missing.to_csv(missing_csv)
print(f"✅ Valores ausentes salvos: {missing_csv.resolve()}")

# Cardinalidade das colunas (número de valores únicos)
card = df.nunique(dropna=False).to_frame("n_unique")
card_csv = OUT_DIR / "cardinalidade.csv"
card.to_csv(card_csv)
print(f"✅ Cardinalidade salva: {card_csv.resolve()}")

# ============================================
# 3) Data Profiling Report — ydata-profiling
# ============================================
profile_html = OUT_DIR / "Relatorio_Automatico_Profiling.html"

try:
    from ydata_profiling import ProfileReport

    extra_args = {}
    if (TARGET_COL is not None) and (TARGET_COL in df.columns):
        extra_args["y"] = TARGET_COL

    profile = ProfileReport(
        df,
        title="Relatório de Data Profiling (EDA)",
        explorative=True,
        **extra_args
    )
    profile.to_file(profile_html)
    print(f"✅ Relatório ydata-profiling salvo: {profile_html.resolve()}")

except Exception as e:
    print("⚠️ Não foi possível gerar o relatório com ydata-profiling.")
    print("   Dica: no Colab, descomente a linha de instalação e rode novamente.")
    print(f"   Erro: {e}")

# ============================================
# 4) Data Profiling Report — Sweetviz
# ============================================
sweetviz_html = OUT_DIR / "Relatorio_Automatico_Sweetviz.html"
try:
    import sweetviz as sv
    target_feat = TARGET_COL if (TARGET_COL is not None and TARGET_COL in df.columns) else None
    rep = sv.analyze(df, target_feat=target_feat)
    rep.show_html(str(sweetviz_html), open_browser=False)
    print(f"✅ Relatório Sweetviz salvo: {sweetviz_html.resolve()}")
except Exception as e:
    print("⚠️ Não foi possível gerar o relatório com Sweetviz.")
    print("   Dica 1: garanta a instalação (descomente o pip acima).")
    print("   Dica 2: se persistir, rode `!pip install -q sweetviz==2.3.1` e reexecute.")
    print(f"   Erro: {e}")

# ============================================
# 5) Para visualizar no notebook / Baixar
# ============================================
try:
    from IPython.display import IFrame, display, HTML
    display(HTML(f"<p><strong>Abrir ydata:</strong> {profile_html}</p>"))
    display(HTML(f"<p><strong>Abrir sweetviz:</strong> {sweetviz_html}</p>"))
    # Descomente uma das duas linhas abaixo para visualizar embutido:
    # display(IFrame(str(profile_html), width="100%", height=800))
    # display(IFrame(str(sweetviz_html), width="100%", height=800))
except Exception:
    pass

# Para baixar do (Colab):
files.download(str(profile_html))
files.download(str(sweetviz_html))
# files.download(str(csv_com_indices))
# files.download(str(desc_csv))
# files.download(str(missing_csv))
# files.download(str(card_csv))

"""### Heatmap de correlação das variáveis numéricas padronizadas

Selecionaremos apenas as colunas numéricas, padronizaremos os dados e exibiremos um heatmap das correlações.

> Auxilia a visualização de tendências nos dados.
"""

# Heatmap de correlação das variáveis numéricas padronizadas
numeric_cols = data.select_dtypes(include="number").columns
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data[numeric_cols].dropna())
scaled_df = pd.DataFrame(scaled_data, columns=numeric_cols)

plt.figure(figsize=(12,8))
sns.heatmap(scaled_df.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Heatmap das variáveis numéricas padronizadas")
plt.show()

"""### Interpretação do Heatmap de Correlação

O gráfico acima mostra a **matriz de correlação** entre as variáveis numéricas da base de dados.

Como interpretar:
- **Diagonal = 1.00**: cada variável correlacionada com ela mesma.
- **Correlação próxima de +1 (vermelho forte)**: relação linear positiva forte.
- **Correlação próxima de -1 (azul forte)**: relação linear negativa forte.
- **Correlação próxima de 0**: pouca ou nenhuma relação linear.

Exemplos observados:
- **Altas correlações positivas (possível multicolinearidade):**
  - `curbweight`, `enginesize`, `horsepower`, `price`, `length`, `width`, `wheelbase`.
  - Todas estão ligadas ao porte do carro e variam juntas.
- **Altas correlações negativas:**
  - `citympg` e `highwaympg` têm correlação negativa com `curbweight`, `enginesize`, `horsepower`.
  - Carros maiores e mais potentes tendem a consumir mais combustível.
- **Correlação fraca:**
  - Variáveis como `stroke` e `compressionratio` mostram baixa correlação com a maioria das outras.

### Heatmap hierárquico das variáveis numéricas padronizadas
"""

# 1) Selecionar e preparar os dados numéricos
numeric_cols = data.select_dtypes(include=np.number).columns

# Tratar NaN (clustermap não aceita NaN). Aqui uso mediana, mas pode trocar para média.
imputer = SimpleImputer(strategy="median")
X = pd.DataFrame(imputer.fit_transform(data[numeric_cols]),
                 columns=numeric_cols, index=data.index)

# Padronizar (média=0, desvio=1)
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X),
                        columns=numeric_cols, index=data.index)

# 2) Clustered heatmap (dados padronizados)
# Este gráfico  é um dendrograma
# e o calor refere-se aos valores padronizados (não à correlação).
sns.set(font_scale=0.8)  # ajuste do tamanho de fonte

g = sns.clustermap(
    X_scaled,
    method="ward",          # método de ligação hierárquica (existem outros métodos como "complete" ou "average" também)
    metric="euclidean",     # métrica de distância mais utiizada
    cmap="YlOrRd",          # paleta de cores
    figsize=(10, 10),
    row_cluster=True,
    col_cluster=True,
    xticklabels=True,       # Se houver muitas colunas pode colocar False
    yticklabels=False,      # True mostra os índices na lateral direita
    dendrogram_ratio=(.1, .2),    # ajusta o espaço dos dendrogramas
    cbar_pos=(.02, .8, .03, .15)  # posição da barra de cores (opcional)
)

plt.title("Heatmap hierárquico das variáveis numéricas padronizadas", y=1.05)
plt.show()

"""### Interpretação do Heatmap Hierárquico

O gráfico acima é um **heatmap hierárquico (clustermap)** gerado a partir das variáveis numéricas padronizadas da base de dados.

Como interpretar:

- **Cores (escala à esquerda):**
  - **Amarelo claro** → valores próximos da média (z-score ≈ 0).
  - **Vermelho/laranja escuro** → valores **acima da média** (z-score positivo).
  - **Tons mais claros/esbranquiçados** → valores **abaixo da média** (z-score negativo).

- **Linhas (cada linha = 1 carro):**
  - Representam os veículos.  
  - Carros semelhantes em suas características são agrupados juntos no dendrograma à esquerda.

- **Colunas (variáveis numéricas):**
  - Variáveis que apresentam comportamento semelhante são agrupadas no dendrograma superior.  
  - Exemplo:
    - `curbweight`, `enginesize`, `horsepower` e `price` aparecem juntas → carros mais pesados tendem a ter motores maiores, mais potência e preço mais alto.
    - `citympg` e `highwaympg` se agrupam → consumo urbano e rodoviário estão fortemente relacionados.

Insights principais:

- Existe um **cluster de variáveis de tamanho/peso/potência/preço**.
- Outro cluster reúne **variáveis de consumo (citympg e highwaympg)**.
- Variáveis como `symboling` e `normalizedlosses` aparecem mais isoladas, pois não têm tanta relação direta com as demais.
- Carros esportivos ou de luxo tendem a aparecer em vermelho nessas variáveis (valores altos), enquanto carros compactos/econômicos aparecem em tons claros.

> Esse tipo de gráfico ajuda a **identificar padrões de semelhança entre carros** e **relações entre variáveis** de forma visual.

### Gráfico de componente principal e a contribuição das variáveis em cada componenete
"""

# X_scaled é o DataFrame já padronizado
pca = PCA()
pca.fit(X_scaled)

# Variância explicada por componente
explained_var = pca.explained_variance_ratio_

# Plot da variância explicada acumulada
plt.figure(figsize=(8,5))
plt.plot(range(1, len(explained_var)+1), explained_var.cumsum(), marker="o")
plt.xlabel("Número de Componentes Principais")
plt.ylabel("Variância Explicada Acumulada")
plt.title("PCA - Variância Explicada")
plt.grid(True)
plt.show()

# Contribuição de cada variável nos 2 primeiros componentes
loadings = pd.DataFrame(
    pca.components_.T,
    columns=[f"PC{i+1}" for i in range(len(explained_var))],
    index=X_scaled.columns
)
print("Contribuição das variáveis no PC1 e PC2:")
print(loadings.iloc[:, :2].sort_values("PC1", ascending=False))

"""### Interpretação dos Resultados da PCA

Variância explicada:
- O eixo X mostra o número de componentes principais (PCs).
- O eixo Y mostra a variância acumulada explicada por esses componentes.

Interpretação:
- PC1 + PC2 + PC3 já explicam aproximadamente 72% da variância da base.
- Com 5 PCs chegamos a aproximadamente 85%.
- Com 7 PCs já temos quase 90%, o que geralmente é ótimo para reduzir dimensionalidade.
- A curva mostra um “cotovelo” por volta do 3º a 5º componente, indicando que além disso os ganhos são marginais.

---

Cargas dos Componentes (loadings: PC1 e PC2):

PC1 (principal eixo da variabilidade)

Mais influentes positivamente:
- curbweight
- length
- width
- enginesize
- price
- horsepower

Mais influentes negativamente:
- citympg
- highwaympg

Interpretação:
- PC1 é um eixo de tamanho/potência/preço.
- Carros maiores, pesados, com motores grandes e caros ficam no extremo positivo.
- Carros pequenos, leves e econômicos ficam no extremo negativo.

PC2 (segundo eixo de variabilidade)

Mais influentes positivamente:
- height
- compressionratio
- wheelbase
- citympg

Mais influentes negativamente:
- symboling
- peakrpm
- normalizedlosses
- horsepower

Interpretação:
- PC2 parece capturar uma dimensão de eficiência versus risco/estilo de motor.
- Positivo: carros mais altos, com maior taxa de compressão e consumo urbano relativamente maior.
- Negativo: carros com maior rotação de pico, maior risco (symboling) e mais perdas normalizadas.

---

Insight geral:
- **PC1** separa os carros grandes/potentes/caros dos compactos/econômicos.
- **PC2** distingue carros por eficiência de motor e perfil de risco.
- Juntos, PC1 e PC2 permitem visualizar bem os grupos de veículos.

Isso reforça o que vimos no heatmap hierárquico:
- Existe um bloco de variáveis fortemente relacionadas ao tamanho e potência.
- E outro bloco de variáveis ligadas à eficiência e consumo.

### PCA com Target *Symboling*
"""

# 1) Seleção e preparo dos dados

# Garante que o target 'symboling' esteja presente
assert 'symboling' in data.columns, "Coluna 'symboling' não encontrada no DataFrame 'data'."

# Mantém apenas colunas numéricas (exceto o target) para a PCA
num_cols = data.select_dtypes(include=np.number).columns.tolist()
num_cols_no_target = [c for c in num_cols if c != 'symboling']

# Imputação (mediana) e padronização (média=0, desvio=1)
imputer = SimpleImputer(strategy="median")
X_num = pd.DataFrame(imputer.fit_transform(data[num_cols_no_target]),
                     columns=num_cols_no_target, index=data.index)

scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_num),
                        columns=num_cols_no_target, index=data.index)

# Remove linhas com target ausente
mask = data['symboling'].notna()
X_scaled = X_scaled.loc[mask]
y = data.loc[mask, 'symboling'].astype(int)

# 2) PCA

pca = PCA(n_components=2)  # para o gráfico PC1 x PC2
Z = pca.fit_transform(X_scaled)

pc_df = pd.DataFrame(Z, columns=['PC1', 'PC2'], index=X_scaled.index)
pc_df['symboling'] = y.values

expl_var = pca.explained_variance_ratio_  # variância explicada por PC

# Cargas (loadings) das variáveis nos PCs
loadings = pd.DataFrame(
    pca.components_.T,
    index=X_scaled.columns,
    columns=['PC1', 'PC2']
).sort_values('PC1', ascending=False)

print("Variância explicada:")
print(f"PC1: {expl_var[0]:.2%} | PC2: {expl_var[1]:.2%} | Acumulada: {(expl_var.sum()):.2%}\n")

print("Top variáveis por PC1 (em módulo):")
print(loadings.reindex(loadings['PC1'].abs().sort_values(ascending=False).index).head(10)[['PC1']])
print("\nTop variáveis por PC2 (em módulo):")
print(loadings.reindex(loadings['PC2'].abs().sort_values(ascending=False).index).head(10)[['PC2']])

# 3) Gráfico PC1 x PC2 colorido por target

plt.figure(figsize=(8,6))
sns.scatterplot(
    data=pc_df, x='PC1', y='PC2', hue='symboling', style='symboling',
    s=60, edgecolor='white', linewidth=0.5, alpha=0.9
)

# Anota variância explicada nos eixos
plt.xlabel(f"PC1 ({expl_var[0]*100:.1f}% var.)")
plt.ylabel(f"PC2 ({expl_var[1]*100:.1f}% var.)")
plt.title("PCA – Dispersão PC1 x PC2 colorida por 'symboling'")
plt.legend(title="symboling", bbox_to_anchor=(1.02, 1), loc="upper left", borderaxespad=0)
plt.grid(True, linewidth=0.3, alpha=0.5)
plt.tight_layout()
plt.show()

# 4) Centroides por classe

centroids = pc_df.groupby('symboling')[['PC1','PC2']].mean()
print("\nCentroides (médias em PC1 e PC2) por 'symboling':")
display(centroids)

"""### Interpretação da PCA com Target *Symboling*

Variância Explicada:
- **PC1: 49,23%**
- **PC2: 16,55%**
- **Acumulada (PC1 + PC2): 65,77%**

Apenas 2 componentes já resumem **2/3 da variabilidade total** dos dados.

---

PC1 – Dimensão Tamanho/Potência x Eficiência:

- **Positivo:** `curbweight`, `length`, `width`, `enginesize`, `price`, `horsepower`
- **Negativo:** `citympg`, `highwaympg`

Interpretação:  
PC1 separa carros **maiores, mais pesados, potentes e caros** (lado positivo) versus **compactos, leves e econômicos** (lado negativo).

---

PC2 – Dimensão Eficiência/Construção x Risco Mecânico:

- **Positivo:** `compressionratio`, `height`, `wheelbase`, `citympg`
- **Negativo:** `peakrpm`, `normalizedlosses`, `horsepower`

Interpretação:  
PC2 diferencia carros **mais altos, eficientes e estáveis** (lado positivo) versus **carros com alta rotação, perdas e maior risco mecânico** (lado negativo).

---

Relação com o Target (*symboling*):

- **-2 e -1 (baixo risco):** PC1 positivo → carros **maiores e mais seguros**  
- **0 (risco neutro):** próximo à origem → perfil intermediário  
- **1 e 2 (risco moderado):** PC1 negativo → **compactos e econômicos**  
- **3 (alto risco):** PC1 positivo mas **PC2 muito negativo** → indicam **instabilidade estrutural/mecânica**

---

Insight Geral:
- **PC1** organiza os veículos pelo **porte, potência e preço**.  
- **PC2** adiciona nuances de **eficiência e risco mecânico**.  
- O *symboling* segue esse gradiente:  
  - **Carros maiores e caros → menor risco**  
  - **Carros compactos e econômicos → maior risco**  
  - **Symboling 3 → perfil de risco extremo, ligado a instabilidade mecânica**

---

> **Conclusão:** A PCA mostra que o risco de seguro `symboling` não é aleatório: ele se alinha com dimensões claras de porte/potência e eficiência mecânica dos veículos.

### Nota sobre Multicolinearidade

A Análise de Componentes Principais (PCA) mostrou como variáveis fortemente correlacionadas podem ser sintetizadas em combinações lineares que explicam grande parte da variância. Esse fenômeno está diretamente relacionado à **multicolinearidade**, que ocorre quando duas ou mais variáveis independentes apresentam alta correlação entre si.  

A multicolinearidade é relevante porque pode:  
1. Reduzir o poder preditivo das variáveis, já que parte da informação se torna redundante.  
2. Aumentar a incerteza das estimativas dos parâmetros (βᵢ), tornando os coeficientes menos estáveis e mais difíceis de interpretar.  

Por isso, antes de ajustar modelos de regressão, é fundamental avaliar a correlação entre as variáveis preditoras. Na próxima aula, veremos como diagnosticar e tratar a multicolinearidade com métricas específicas, como o **Fator de Inflação da Variância (VIF)**.  

> O **VIF (Variance Inflation Factor)** quantifica o quanto a variância de um coeficiente de regressão é inflacionada pela presença de multicolinearidade.

